{#
  Copyright 2025 The Torch-Spyre Authors.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
#}
{{signature_out}} spyre__{{ template_data.op_name }}({{signature_in}}) {
  DEBUGINFO("Tensor is on: ", {{ arguments[0].name }}.device());
  {% for arg in arguments[1:] %}
  {% if ("Tensor" in arg.type and "Tensor" in arguments[0].type) %}
  // TORCH_INTERNAL_ASSERT({{ arguments[0].name }}.device() == {{ arg.name }}.device());
  {% endif %}
  {% endfor %}

  {% if "out" in overload_name %}
    {% set out_given = True %}
    {% set output_variable_name = "out" %}
    {% set arguments = arguments[:-1] %}
  {% else %}
    {% set out_given = False %}
    {% if returns|length > 0 %}
      {% set output_variable_name = returns[0].name %}
    {% else %}
      {% if inplace %}
      {% set output_variable_name = arguments[0].name %}
      {% else %}
      {% set output_variable_name = "out" %}
      {% endif %}
    {% endif %}
  {% endif %}

  {% if not out_given and not inplace %}
  {% if out_shape_stride_expr == "bypass" %}
  auto {{ output_variable_name }} = spyre_empty_strided(
    {{ arguments[0].name }}.sizes(),
    {{ arguments[0].name }}.strides(),
    {{ arguments[0].name }}.scalar_type(),
    {{ arguments[0].name }}.layout(),
    {{ arguments[0].name }}.device(),
    {{ arguments[0].name }}.is_pinned()
  );
  {% elif out_shape_stride_expr == "infer" %}
  std::vector<int64_t> out_shape;
  std::vector<int64_t> out_strides;
  {% set num_max_return_dims = returns[0].shape|length %}
  {% for i in range(num_max_return_dims) %}
  if ({{ returns[0].shape[i].split('.')[0] }}.dim() >= {{ num_max_return_dims - i }}) {
    out_shape.push_back({{ returns[0].shape[i] }});
    out_strides.push_back({{ returns[0].stride[i] }});
  }
  {% endfor %}
  auto {{ output_variable_name }} = spyre_empty_strided(
    out_shape,
    out_strides,
    {{ arguments[0].name }}.scalar_type(),
    {{ arguments[0].name }}.layout(),
    {{ arguments[0].name }}.device(),
    {{ arguments[0].name }}.is_pinned()
  );
  {% else %}
  {% if out_shape_stride_expr == "reduce" %}
  auto out_shape_stride = getOutputShapeStrideForReduce({{ arguments[0].name }}.sizes(), dim, keepdim);
  {% elif out_shape_stride_expr == "matmul" %}
  {% if "addmm" in template_data.op_name %}
  auto out_shape_stride = getOutputShapeStrideForMatmul({{ arguments[1].name }}.sizes(), {{ arguments[2].name }}.sizes());
  {% else %}
  auto out_shape_stride = getOutputShapeStrideForMatmul({{ arguments[0].name }}.sizes(), {{ arguments[1].name }}.sizes());
  {% endif %}
  {% elif out_shape_stride_expr == "transpose" %}
  auto out_shape_stride = getOutputShapeStrideForTranspose({{ arguments[0].name }}.sizes(), {{ arguments[0].name }}.strides(), dim0, dim1);
  {% elif out_shape_stride_expr == "concat" %}
  std::vector<at::IntArrayRef>inp_sizes_vector;
  for (const auto& tensor: {{ arguments[0].name }}) {
    inp_sizes_vector.push_back(tensor.sizes())
  }
  auto out_shape_stride = getOutputShapeStrideForConcat(inp_sizes_vector, dim);
  {% endif %}
  auto {{ output_variable_name }} = spyre_empty_strided(
    out_shape_stride.first,
    out_shape_stride.second,
    {{ arguments[0].name }}.scalar_type(),
    {{ arguments[0].name }}.layout(),
    {{ arguments[0].name }}.device(),
    {{ arguments[0].name }}.is_pinned()
  );
  {% endif %}
  {% endif %}

  std::string graph_label = {{ template_data.op_label }};
  {% for idx in sendnn_arg_order_list %}
    {% if arguments[idx].sendnn_type == "ConstInput" %}
      {% if ('optional' in arguments[idx].type or 'Optional' in arguments[idx].type) %}
  graph_label += "__" + std::to_string(*{{ arguments[idx].name }});
      {% else %}
  graph_label += "__" + std::to_string({{ arguments[idx].name }});
      {% endif %}
    {% elif arguments[idx].sendnn_type == "TensorShape" %}
      {% if arguments[idx].type == "int64_t" %}
  graph_label += "__" + std::to_string({{ arguments[idx].name }});
      {% else %}
  if ({{ arguments[idx].name }}.has_value()) {
    graph_label += "_";
    for (size_t j=0; j < dim->size(); j++)
      graph_label += "_" + std::to_string({{ arguments[idx].name }}->at(j));
  }
      {% endif %}
    {% elif arguments[idx].sendnn_type in ["float", "int", "double"] %}
      {% if ('optional' in arguments[idx].type or 'Optional' in arguments[idx].type) %}
  graph_label += "__" + std::to_string((*{{ arguments[idx].name }}));
      {% else %}
  graph_label += "__" + std::to_string({{ arguments[idx].name }});
      {% endif %}
    {% elif arguments[idx].sendnn_type == "Default" %}
  graph_label += "__" + std::to_string({{ arguments[idx].default }});
    {% elif arguments[idx].sendnn_type == "ByPass" %}
      {% if ('optional' in arguments[idx].type or 'Optional' in arguments[idx].type) %}
  graph_label += "__" + std::to_string(*{{ arguments[idx].name }});
      {% else %}
  graph_label += "__" + std::to_string({{ arguments[idx].name }});
      {% endif %}
    {% endif %}
  {% endfor %}

  DEBUGINFO("Operating on graph with label: ", graph_label);

  std::optional<sendnn::GraphLoader> opt_gl = spyre::getCachedGraphLoader(graph_label, {{ output_variable_name }}.sizes(), {{ output_variable_name }}.strides());

  {% set eager_inp = namespace(counter=arguments|length) %}
  sendnn::GraphLoader gl;
  if (opt_gl.has_value()) {
    gl = opt_gl.value();
  } else {
    sendnn::GraphBuilder gb;
  
    {% set eager_inp.counter = 0 %}
    {% for idx in sendnn_arg_order_list %}
      {% if arguments[idx].sendnn_type == "PrimaryInput" %}
        {% set eager_inp.counter = eager_inp.counter + 1 %}
        {% if ('optional' in arguments[idx].type or 'Optional' in arguments[idx].type) %}
    sendnn::TensorInfo ti_{{ loop.index }} = getTensorInfo(*{{ arguments[idx].name }});
        {% else %}
    sendnn::TensorInfo ti_{{ loop.index }} = getTensorInfo({{ arguments[idx].name }});
        {% endif %}
    auto inp_{{ loop.index }} = gb.PrimaryInput("Input{{ loop.index }}", ti_{{ loop.index }});
      {% elif arguments[idx].sendnn_type == "PrimaryInputScalar" %}
    sendnn::TensorInfo ti_{{ loop.index }} = getScalarTensorInfo({{ arguments[0].name }});
    auto inp_{{ loop.index }} = gb.PrimaryInput("Input{{loop.index}}", ti_{{ loop.index }});
      {% elif arguments[idx].sendnn_type == "ConstInput" %}
    sendnn::TensorInfo ti_{{ loop.index }} = getScalarTensorInfo({{ arguments[0].name }});
        {% if ('optional' in arguments[idx].type or 'Optional' in arguments[idx].type) %}
    sendnn::ConstTensor ct_{{ loop.index }} = sendnn::ConstTensor(ti_{{ loop.index }}, *{{ arguments[idx].name }});
        {% else %}
    sendnn::ConstTensor ct_{{ loop.index }} = sendnn::ConstTensor(ti_{{ loop.index }}, &{{ arguments[idx].name }});
        {% endif %}
    auto inp_{{ loop.index }} = gb.ConstInput("Input{{loop.index}}", ct_{{ loop.index }});
      {% elif arguments[idx].sendnn_type == "TensorShape" %}
        {% if arguments[idx].type == "int64_t" %}
    std::vector<int64_t> shape_{{ loop.index }} = { {{ arguments[idx].name }} };
        {% else %}
    std::vector<int64_t> shape_{{ loop.index }} = { };
    if ({{ arguments[idx].name }}.has_value()) {
      for (size_t j=0; j < dim->size(); j++)
        shape_{{ loop.index }}.push_back({{ arguments[idx].name }}->at(j));
    }
        {% endif %}
    sendnn::TensorShape inp_{{ loop.index }}(shape_{{ loop.index }});
      {% elif arguments[idx].sendnn_type in ["float", "int", "double"] %}
        {% if ('optional' in arguments[idx].type or 'Optional' in arguments[idx].type) %}
    auto inp_{{ loop.index }} = static_cast<{{ arguments[idx].sendnn_type }}>(*{{ arguments[idx].name }});
        {% else %}
    auto inp_{{ loop.index }} = static_cast<{{ arguments[idx].sendnn_type }}>({{ arguments[idx].name }});
        {% endif %}
      {% elif arguments[idx].sendnn_type == "Default" %}
    auto inp_{{ loop.index }} = {{ arguments[idx].default }};
      {% elif arguments[idx].sendnn_type == "ByPass" %}
        {% if ('optional' in arguments[idx].type or 'Optional' in arguments[idx].type) %}
    auto inp_{{ loop.index }} = *{{ arguments[idx].name }};
        {% else %}
    auto inp_{{ loop.index }} = {{ arguments[idx].name }};
        {% endif %}
      {% endif %}
    {% endfor %}

    sendnn::TensorInfo ti = getTensorInfo({{ output_variable_name }});

    auto r = gb.{{ template_data.sendnn_func_name }}({{ template_data.op_label }}, ti{% for idx in sendnn_arg_order_list %}, inp_{{ loop.index }}{% endfor %});
    gb.PrimaryOutput("Output", r);

    {% set sn_idx = [[eager_inp.counter, 1] | max, 2] | min - 1 %}
    gl = prepareGraphLoader(&gb);
    gl = parseGraphLoader(gl);
    {% if sn_idx == 0 %}
    auto predict_s = gl.Predict(sendnn::Outputs(), sendnn::Inputs());
    {% else %}
    auto predict_s = gl.Predict(sendnn::Outputs(), {createInputTensor(gl, {{ arguments[1].name }}.storage().data_ptr().get())}, 1);
    {% endif %}
    storeCachedGraphLoader(graph_label, {{ output_variable_name }}.sizes(), {{ output_variable_name }}.strides(), gl);
  }

  std::vector<sendnn::ConstTensor> input_sendnn_tensor_vector;
  {% for idx in sendnn_arg_order_list %}
  at::Tensor tmp_{{ loop.index0 }};
  {% if arguments[idx].sendnn_type == "PrimaryInput" %}
  if ({{ arguments[idx].name }}.dim() == 0) {
    tmp_{{ loop.index0 }} = (at::ones({1}, {{ arguments[0].name }}.dtype()) * {{ arguments[idx].name }}).to({{ arguments[0].name }}.device());
    auto& input_sendnn_tensor = input_sendnn_tensor_vector.emplace_back(createInputTensor(gl, tmp_{{ loop.index0  }}.storage().data_ptr().get(), {{idx}}, {{ sn_idx + 1 }}));
    input_sendnn_tensor.SetSpyreData((static_cast<SharedOwnerCtx*>(tmp_{{ loop.index0  }}.storage().data_ptr().get_context()))->owner);
  }
  else {
    auto& input_sendnn_tensor = input_sendnn_tensor_vector.emplace_back(createInputTensor(gl, {{ arguments[idx].name }}.storage().data_ptr().get(), {{idx}}, {{ sn_idx + 1 }}));
    input_sendnn_tensor.SetSpyreData((static_cast<SharedOwnerCtx*>({{ arguments[idx].name }}.storage().data_ptr().get_context()))->owner);
  }
  {% elif arguments[idx].sendnn_type == "PrimaryInputScalar" %}
  tmp_{{ loop.index0 }} = (at::ones({1}, {{ arguments[0].name }}.dtype()) * {{ arguments[idx].name }}).to({{ arguments[0].name }}.device());
  auto& scalar_sendnn_tensor = input_sendnn_tensor_vector.emplace_back(createInputTensor(gl, tmp_{{ loop.index0  }}.storage().data_ptr().get(), {{idx}}, {{ sn_idx + 1 }}));
  scalar_sendnn_tensor.SetSpyreData((static_cast<SharedOwnerCtx*>(tmp_{{ loop.index0  }}.storage().data_ptr().get_context()))->owner);
  {% endif %}
  {% endfor %}
  
  auto output_sendnn_tensor = createOutputTensor(gl, {{ output_variable_name }}.storage().data_ptr().get(), 0, {{ sn_idx + 1 }});
  output_sendnn_tensor.SetSpyreData((static_cast<SharedOwnerCtx*>({{ output_variable_name }}.storage().data_ptr().get_context()))->owner);

  auto copy_status = gl.Compute({output_sendnn_tensor}, input_sendnn_tensor_vector, {{ sn_idx + 1 }});

  {% if returns|length > 0 %}
  return {{ returns[0].name }};
  {% endif %}
}

TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {
  m.impl({{ template_data.reg_name }}, TORCH_FN(spyre__{{ template_data.op_name }}));
}
